Sections

SEARCH

\protect\hyperlink{site-content}{Skip to
content}\protect\hyperlink{site-index}{Skip to site index}

\href{https://www.nytimes3xbfgragh.onion/podcasts/the-daily}{The Daily}

\href{https://myaccount.nytimes3xbfgragh.onion/auth/login?response_type=cookie\&client_id=vi}{}

\href{https://www.nytimes3xbfgragh.onion/section/todayspaper}{Today's
Paper}

\href{/podcasts/the-daily}{The Daily}\textbar{}Wrongfully Accused by an
Algorithm

\href{https://nyti.ms/3jZGj9I}{https://nyti.ms/3jZGj9I}

\begin{itemize}
\item
\item
\item
\item
\item
\item
\end{itemize}

Advertisement

\protect\hyperlink{after-top}{Continue reading the main story}

transcript

Back to The Daily

bars

0:00/28:13

-28:13

transcript

\hypertarget{wrongfully-accused-by-an-algorithm}{%
\subsection{Wrongfully Accused by an
Algorithm}\label{wrongfully-accused-by-an-algorithm}}

\hypertarget{hosted-by-annie-brown-produced-by-lynsea-garrison-austin-mitchell-and-daniel-guillemette-and-edited-by-lisa-tobin-and-larissa-anderson}{%
\subsubsection{Hosted by Annie Brown, produced by Lynsea Garrison,
Austin Mitchell and Daniel Guillemette, and edited by Lisa Tobin and
Larissa
Anderson}\label{hosted-by-annie-brown-produced-by-lynsea-garrison-austin-mitchell-and-daniel-guillemette-and-edited-by-lisa-tobin-and-larissa-anderson}}

\hypertarget{in-what-may-be-the-first-known-case-of-its-kind-a-faulty-facial-recognition-match-led-to-a-michigan-mans-arrest-for-a-crime-he-did-not-commit}{%
\paragraph{In what may be the first known case of its kind, a faulty
facial recognition match led to a Michigan man's arrest for a crime he
did not
commit.}\label{in-what-may-be-the-first-known-case-of-its-kind-a-faulty-facial-recognition-match-led-to-a-michigan-mans-arrest-for-a-crime-he-did-not-commit}}

Monday, August 3rd, 2020

\begin{itemize}
\item
  michael barbaro\\
  From The New York Times, I'm Michael Barbaro. This is ``The Daily.''
\item
  {[}music{]}\\
  Today: Facial recognition is becoming an increasingly popular tool for
  solving crimes. The Daily's Annie Brown speaks to Kashmir Hill about
  how that software is not treating everybody equally.

  It's Monday, August 3.
\item
  kashmir hill\\
  I'm just going the tape record with an app that I use. Do you guys
  have any questions or concerns before we start talking about what
  happened?
\item
  robert williams\\
  No.
\item
  melissa williams\\
  No.
\end{itemize}

annie brown

OK. So where do you think we should start the story of this case,
Kashmir?

kashmir hill

The story started, for the Williams family, in January of 2020.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Melissa got the call first. I got the call from her.
\end{itemize}

kashmir hill

It's a Thursday afternoon in Farmington Hills, Michigan, which is just
outside of Detroit.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  So I picked up Julia from school. Regular Thursday.
\end{itemize}

kashmir hill

And Melissa Williams, a realtor, is driving home from work. She's
picking up her daughter.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  And so it was right around, like, 4 o'clock. And I got a call.
\end{itemize}

kashmir hill

And she gets a call from somebody who says they're a police officer.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  They immediately said, we're calling about Robert from an incident in
  2018. He needs to turn himself in. So I was confused off the bat.
\end{itemize}

kashmir hill

She is white. And her husband, Robert Williams, is Black.

\begin{itemize}
\item
  melissa williams\\
  And they said, we assume you're his baby mama or that you're not
  together anymore. And ---
\item
  kashmir hill\\
  What?
\item
  melissa williams\\
  Yeah. I said, that's my husband. And what is this regarding? And they
  said, we can't tell you. But he needs to come turn himself in. And I
  said, well, why didn't you call him? And they said, can't you just
  give him a message?
\end{itemize}

annie brown

Wait. So why is this officer calling her?

kashmir hill

She doesn't know why the officer is calling her. All she knows is that
the police want to be in touch with her husband. So she gives the
officer her husband's number. And then she calls Robert.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  And I said, I just got a really weird call. I was like, what did you
  do? Like, what is this about?
\end{itemize}

kashmir hill

And while they're talking, Robert Williams gets a call from the police
department.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Of course, I answered the other line. And he said he was a detective
  from Detroit and that I need to come turn myself in. So of course I'm
  like, for what? And he's like, well, I can't tell you over the phone.
  So I'm like, well, I can't turn myself in then.
\end{itemize}

kashmir hill

It was a couple of days before his birthday. So he thought maybe it was
a prank call. But it became pretty clear that the person was serious.

\begin{itemize}
\tightlist
\item
  robert williams\\
  About, uh, probably ten minutes later, I pull in the driveway.
\end{itemize}

kashmir hill

And when he pulls into his driveway, a police car pulls in behind him,
blocking him in. And two officers get out.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Yeah. So I get out of the car. And the driver, like, runs up. And he's
  like, are you Robert Williams? I'm like, yeah. He's like, you're under
  arrest. I'm like, no I'm not. And the guy comes up with, like, a white
  sheet of paper. And it's said ``felony warrant'' on the top,
  ``larceny.'' And I'm confused, like, isn't larceny stealing?
\end{itemize}

kashmir hill

His wife comes out with his two young daughters. And his oldest
daughter, who's 5, is watching this happen.

\begin{itemize}
\tightlist
\item
  robert williams\\
  I said, Juju (ph), go back in the house. I'll be back in a minute.
  They're just making a mistake. The guy, the other cop, is behind me
  with his handcuffs out already. So he's like, come on, man. You
  already --- you know the drill. And I'm like, what?
\end{itemize}

kashmir hill

The officers arrest him. They have to use two pairs of handcuffs to get
his hands behind his back, because he's a really big guy.

\begin{itemize}
\tightlist
\item
  robert williams\\
  We started moving seats around, trying to get me in the back of this
  little bitty Impala. And off we go.
\end{itemize}

kashmir hill

And then they drive to the detention center.

{[}music{]}

\begin{itemize}
\item
  robert williams\\
  I took fingerprints. I took ---
\item
  kashmir hill\\
  Your mug shot.
\item
  robert williams\\
  Mug shot pictures.
\end{itemize}

kashmir hill

Then he's put in a cell to sleep overnight.

\begin{itemize}
\item
  robert williams\\
  At this point, I'm in a holding cell with two other guys. And they're
  like, what you in here for? And I'm like, I don't know.
\item
  kashmir hill\\
  So when do you actually find out why you've been arrested, beyond this
  kind of vague larceny?
\item
  robert williams\\
  Um, so --- well, maybe like noon the next day.
\end{itemize}

kashmir hill

Around noon the next day, he is taken to an interrogation room. And
there's two detectives there. And they have three pieces of paper face
down in front of them. And they turn over the first sheet of paper. And
it's a picture from a surveillance video of a large Black man standing
in a store, wearing a red Cardinals cap and a black jacket. And the
detectives ask, is this you?

\begin{itemize}
\tightlist
\item
  robert williams\\
  I laugh a little bit, and I say, no, that's not me. So then he turns
  over another paper.
\end{itemize}

kashmir hill

And they turn over a second piece of paper, which is just a close up of
that same guy's face.

\begin{itemize}
\tightlist
\item
  robert williams\\
  And he says I guess that's not you either. And I said, no. This is not
  me.
\end{itemize}

kashmir hill

So Robert picks the piece of paper up, holds it next to his own face ---

\begin{itemize}
\tightlist
\item
  robert williams\\
  I was like, what you think, all Black men look alike?
\end{itemize}

kashmir hill

--- and says, do all Black men look the same to you?

annie brown

So what's your understanding, Kashmir, of what happened to bring Robert
Williams into that police department?

kashmir hill

So Robert Williams had no idea what was happening. But two years
earlier, in October 2018, a man who was not him walked into a Shinola
store in downtown Detroit. And Shinola is kind of like a high-end store
that sells expensive watches and bikes. So this man came in. He was
there for a few minutes. He stole five watches worth \$3,800 and walked
out. None of the employees there actually saw the theft occur. And so
they had to review the surveillance footage. And they found the moment
it happened. So they sent that surveillance footage picture that Robert
Williams had been shown to the Detroit police. And the police turned to
what a lot of police turn to these days when they have a suspect that
they don't recognize --- a facial recognition system. So they ran a
search on this, what they call a probe image, this picture for the
surveillance video, which is really grainy. It's not a very good photo.
And the way these systems work is that they have access not just to mug
shots but also to driver's license photos. You get a bunch of different
results. And there's a human involved who decides which of the results
looks the most like the person who committed the crime.

annie brown

Mm. So you're saying the facial recognition algorithm basically created
a lineup of potential suspects. And then from that lineup, someone picks
the person that they think looks the most like the man in the
surveillance video.

kashmir hill

Right. And so that is how they wound up arresting Robert Williams.

{[}music{]}

So back in this room, the two detectives now have the real Robert
Williams in front of them. And he doesn't look like this guy.

\begin{itemize}
\tightlist
\item
  robert williams\\
  You know, they sat back and looked at each other and was like, with
  the oops face, right? Says, so I guess the computer got it wrong too.
\end{itemize}

kashmir hill

And so they kind of leaned back and said, I guess the computer got it
wrong.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Well, the computer got it wrong is what threw me off. And I'm like,
  computer got it wrong?
\end{itemize}

annie brown

And what is the significance of that statement, ``that the computer got
it wrong``?

kashmir hill

So this was an admission by the detectives that it was a computer that
had pointed the finger at Robert Williams. And that's significant,
because this is the first documented case of an innocent person being
arrested because of a flawed facial recognition match.

{[}music{]}

annie brown

And just to put all of this into context for a second, the last time
that you and I talked, Kashmir, we were talking about a different
development in facial recognition --- this new algorithm being used by
some police departments that drew from pictures all over social media
and all over the internet to make a kind of super algorithm. But the
fear wasn't that it wasn't accurate. It was almost that it was too
accurate, that it knew too much. But what you're describing is something
altogether different. Right?

kashmir hill

So when we talk about facial recognition, we often think of it as a
monolith, that there's kind of one facial recognition. But in fact,
there's a bunch of different companies that all have their own
algorithms. And some work well. And some don't work well. And some work
well sometimes. Like, identifying a really clear photo is a lot easier
than identifying surveillance footage.

annie brown

And why wouldn't police departments be using the most sophisticated, the
most kind of up-to-date version of this software?

kashmir hill

I mean, this is where you run into just bureaucracy. Right? You have
contracts with companies that go back years and just a lot of different
vendors. And so in this case, I tried to figure out exactly whose
algorithms were responsible for Robert Williams getting arrested. And I
had to really dig down. And I discovered the police had no idea. You
know, they contract out to a company called DataWorks Plus. And
DataWorks Plus contracts out to two other companies called N.E.C. and
Rank One that actually supply the algorithm. It's this whole chain of
companies that are involved. And there is no standardized testing.
There's no one really regulating this. There's just nobody saying which
algorithms, you know, pass the test to be used by law enforcement. It's
just up to police officers, who, for the most part, seem to be just
testing it in the field to see if it works, if it's identifying the
right people.

But the really big problem is that these systems have been proven to be
biased.

{[}music{]}

michael barbaro

We'll be right back.

annie brown

So, Kashmir, help me understand how an algorithm can become biased.

kashmir hill

Well, the bias tends to come from how the algorithm is trained. And
these algorithms tend to be trained by basically feeding them with a
bunch of images of people. But the problem with the algorithms is that
they tended to be trained with non-diverse data sets.

annie brown

Mm.

kashmir hill

So one good example is that many of the algorithms used by law
enforcement in the U.S., by government in the U.S., are very good at
recognizing white men and not as good at recognizing Black people or
Asian-Americans. But if you go to an algorithm from a company in China,
where they fed it with a lot of images of Asian people, they're really
good at recognizing Asian people and not as good at recognizing white
men. So you can just, you can see the biases that come in from the kind
of data that we feed into these systems.

annie brown

And is this a widely agreed upon reality --- that because of these
methods, the algorithms used in the U.S. are just worse at identifying
faces that aren't white men?

kashmir hill

Yeah. A few years ago, an M.I.T. researcher did this study and found
that facial recognition algorithms were biased to be able to recognize
white men better. And shortly after that, NIST, the National Institute
of Standards and Technology, decide to run its own study on this. And it
found the same thing. It looked at over 100 different algorithms. And it
found that they were biased. And actually, the two algorithms that were
at the heart of this case --- the Robert Williams's case --- were in
that study.

annie brown

So the algorithm that was used by this police department was actually
studied by the federal government and was proven to be biased against
faces like Robert Williams.

kashmir hill

Exactly.

annie brown

So given these widely-understood problems with these algorithms, how can
police departments justify continuing to use them?

kashmir hill

So police departments are aware of the bias problem. But they feel that
face recognition is just too valuable a tool in their tool set to solve
crimes. And their defense is that they never arrest somebody based on
facial recognition alone, that facial recognition is only what they call
an investigative lead. It doesn't supply probable cause for arrest.

So what police are supposed to do is they get a facial recognition
match, and you're supposed to do more investigating. So you could go to
the person's social media account and see if there are other photos of
them wearing the same clothes that they were wearing on the day they
committed this crime. Or, you know, you can try to get proof that they
were in that part of town on the day that the theft occurred. You know,
try to get location data. Basically, find other evidence that this
person is the person that committed the crime.

The detectives just went to the woman who had spotted the theft on the
video and showed her a photo of six people --- they call it a six pack.
And she said Robert Williams looked the most like the person that was in
the video.

annie brown

Mm. So they're supposed to use the facial recognition match as a kind of
clue. And then the protocol calls for them to do more police work to
verify it. But in this case, they basically just had someone watch the
video and then identify Robert Williams as the one who looks most like
the guy in the video.

kashmir hill

Yeah, they just did facial recognition a second time, but with a human
who's not actually trained. And they didn't do any other investigating.
Based on that, they went out and they arrested Mr. Williams.

annie brown

But if the police had done their job correctly --- if they had looked
into his social media accounts, if they had tried to get his location
data from his phone records, essentially surveilling him more closely
--- wouldn't that be its own sort of violation? Just because their
technology wrongfully identified this man, he gets more closely watched
by the police without his knowledge.

kashmir hill

Right. And this is actually what police asked the facial recognition
vendors to do. They want to have more, what you call false positives,
because they want to have the greatest pool of possible suspects that
they can, because they want to find the bad guy.

annie brown

Huh.

kashmir hill

But there's a real toll from that.

annie brown

Hmm.

kashmir hill

I just, you know, as a person who's been reporting on technology for a
decade, I just think people trust computers. And even when we know
something is flawed, if it's a computer telling us to do it, we just
think it's right. And this is why we always used to see, for a long
time, when mapping technology was first being developed and it wasn't
that great, you know, people would drive into lakes. They would drive
over cliffs, because a mapping app said, you're supposed to go straight
here.

annie brown

Right.

kashmir hill

And even though they could look and see that their life is going to be
in danger, they would think, well, this app must know what it's talking
about. That's facial recognition now. And when I was reporting this
story, all the experts I talked to said this is surely not the first
case where somebody has been mistakenly --- an innocent person has been
mistakenly arrested because of a bad face recognition match. But usually
people don't find out about it. Police don't tell people that they're
there because of face recognition.

annie brown

Hmm.

kashmir hill

Usually, when they charge them, they'll just say they were identified
through investigative means. It's kind of a vague, ``There were clues
that pointed at you.'' In that way, Robert's case was unusual, because
there was so little evidence against him. They basically had to tell him
that they used facial recognition, you know, to put him there.

annie brown

Right. They showed him what most people don't get to see, which is this
false match between his photo and the photo of the crime.

kashmir hill

Right.

annie brown

And what's happened since Robert was arrested?

kashmir hill

So Robert had to hire a lawyer to defend himself. But when he went to
the hearing, the prosecutor decided to drop the case. But they dropped
it without prejudice, which meant that they could charge him again.

annie brown

For the same crime?

kashmir hill

With the same crime. So as I was reporting out the story, you know, I
went to the prosecutor's office. I went to the Detroit Police
Department. And I said, you know, what happened here? Did you have any
other evidence? This just seems like a clear misfire and misuse of
facial recognition. And everyone involved was pretty defensive and said,
well, you know, there might be more evidence that proves that Robert
Williams did it.

But after the story came out, everybody's tune changed dramatically.
Prosecutors office apologized, said that Robert Williams shouldn't have
spent any time in jail. The Detroit Police Department said this was a
horrible investigation. The police officers involved just did this all
wrong. This isn't how it's supposed to work. And they said that Robert
Williams would have his information expunged from the system --- his mug
shot, his DNA. And they personally apologized to the Williams family,
though the Williams family told me that no one ever actually called them
to personally apologize.

annie brown

But he can no longer be charged in the future for this crime?

kashmir hill

That's exactly right.

annie brown

And what about their use of facial recognition software? Has there been
any change there?

kashmir hill

So one thing the Detroit Police Department said was, well, this was a
case that predates this new policy we have that says, you know, we're
only supposed to be using facial recognition for violent crimes.

annie brown

Hmm. And what do you make of that? Why only use this tool for that?

kashmir hill

Well, their justification is that when it comes to violent crimes, when
it comes to murder, you know, rape, they need to solve these cases. And
they'll use any clue they can to do it, including facial recognition.
But I think about something that Robert's wife said.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  When they pulled up to our house, they were already combative on the
  phone. They were aggressive in the doorway to me. What if he had been
  argumentative? If he'd been defensive, if he hadn't complied, you
  know, what could that have turned into in our yard? Like, it could
  have went a different way. And the recent news has shown us that it
  definitely could have went a different way.
\end{itemize}

{[}music{]}

\begin{itemize}
\item
  kashmir hill\\
  Do you feel like there's a shame to this, that the police arrested you
  even though you did nothing?
\item
  robert williams\\
  It's a little humiliating. You know, it's not something that easily
  rolls off the tongue, like, oh yeah, and guess what? I got arrested.
\end{itemize}

{[}music{]}

annie brown

And what about for Robert himself? What has life been like for him after
the arrest?

kashmir hill

So this was very embarrassing for him and kind of painful in some ways.
So he had a perfect attendance at work until that day that he was
arrested. And his wife had to email his boss and say that they had a
family emergency and that he couldn't show up that day. Once he did tell
his boss what happened, his boss said, you know, you don't want to tell
other people at work. You know, it could be bad for you. The night he
got home, his daughter --- his 5-year-old was still awake.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Julia was still up. And I was like, what are you doing up? And she was
  like, I'm waiting for you. And I was like, I told you I'll be right
  back. And she was like, you didn't come right back though. So I just
  kept telling her that they made a mistake. And it just took longer
  than we expected. But ---
\end{itemize}

kashmir hill

She started wanting to play cops and robbers. And she would always
pretend like he was the robber who stole something, and she would need
to lock him up in the living room.

annie brown

Hmm.

\begin{itemize}
\item
  melissa williams\\
  Oh yeah. She told us that she told one of her --- Jackson, her friend
  at school. And we weren't sure, did she tell her teacher? Did she tell
  her friends? We were not sure. And we didn't know what to say to
  people. Like, just bring it up out of nowhere, like, oh yeah, in case
  anyone mentioned it, he was arrested, but he didn't do anything.
\item
  kashmir hill\\
  Has this made you look back to see where you --- like, where you were
  October 2018?
\item
  robert williams\\
  Yeah. I pulled it up. At the time, I was on my Facebook or on my
  Instagram Live.
\end{itemize}

kashmir hill

He has since looked back and realized that he had posted to Instagram at
basically the same time as the shoplifting was occurring. He was driving
home from work, and a song came on the radio that his mother loved: the
song ``We Are One'' by Maze and Frankie Beverly.

\begin{itemize}
\tightlist
\item
  robert williams\\
  I was singing songs on my way home in the car.
\end{itemize}

annie brown

So if the cops had looked in to his social media, if they had tried to
verify that it was possible that he could have committed this crime,
they could have found this video.

kashmir hill

Right. If the police had done a real investigation, they would have
found out he had an alibi that day.

\begin{itemize}
\tightlist
\item
  archived recording\\
  {[}``WE ARE ONE'' PLAYING{]}
\end{itemize}

annie brown

Kashmir, thank you so much.

kashmir hill

Thank you.

{[}music{]}

michael barbaro

We'll be right back.

Here's what else you need to know today. Federal unemployment benefits
have expired for tens of millions of Americans after Congress failed to
reach a deal to renew them last week.

\begin{itemize}
\item
  archived recording\\
  So what do you say to those 30 million Americans who are now without
  federal unemployment help?
\item
  archived recording (nancy pelosi)\\
  I say to them, talk to President Trump. He's the one who is standing
  in the way of that. We have been for the \$600. They have a \$200
  proposal, which does not meet the needs of America's working families.
  And ---
\end{itemize}

michael barbaro

In interviews on Sunday with ABC's ``This Week,'' House Speaker Nancy
Pelosi blamed Republicans for demanding a drastic cut in the weekly
benefit, while Treasury Secretary Steve Mnuchin claimed that the \$600
payments risked overpaying unemployed workers.

\begin{itemize}
\item
  archived recording\\
  So you do think it is a disincentive to find a job if you have that
  extra \$600?
\item
  archived recording (steven mnuchin)\\
  There's no question. In certain cases where we're paying people more
  stay home than to work, that's created issues in the entire economy.
\end{itemize}

michael barbaro

And The Times reports that July was a devastating month for the pandemic
in the U.S. The country recorded nearly 2 million new infections, twice
as many as any previous month.

\begin{itemize}
\tightlist
\item
  archived recording (deborah birx)\\
  I want to be very clear. What we're seeing today is different from
  March and April. It is extraordinarily widespread. It's into the rural
  as equal urban areas.
\end{itemize}

michael barbaro

In an interview on Sunday with CNN, Dr. Deborah Birx, a top White House
adviser on the pandemic, acknowledged that the United States has failed
to contain the virus.

\begin{itemize}
\tightlist
\item
  archived recording (deborah birx)\\
  And to everybody who lives in a rural area, you are not immune or
  protected from this virus. And that's why we keep saying, no matter
  where you live in America, you need to wear a mask and socially
  distance. Do the personal hygiene ---
\end{itemize}

michael barbaro

That's it for ``The Daily.'' I'm Michael Barbaro. See you tomorrow.

{[}music{]}

\href{https://www.nytimes3xbfgragh.onion/column/the-daily}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2017/01/29/podcasts/the-daily-album-art/the-daily-album-art-square320-v4.png}The
Daily}Subscribe:

\begin{itemize}
\tightlist
\item
  \href{https://itunes.apple.com/us/podcast/id1200361736}{Apple
  Podcasts}
\item
  \href{https://www.google.com/podcasts?feed=aHR0cHM6Ly9yc3MuYXJ0MTkuY29tL3RoZS1kYWlseQ\%3D\%3D}{Google
  Podcasts}
\end{itemize}

\hypertarget{wrongfully-accused-by-an-algorithm-1}{%
\section{Wrongfully Accused by an
Algorithm}\label{wrongfully-accused-by-an-algorithm-1}}

\hypertarget{in-what-may-be-the-first-known-case-of-its-kind-a-faulty-facial-recognition-match-led-to-a-michigan-mans-arrest-for-a-crime-he-did-not-commit-1}{%
\subsection{In what may be the first known case of its kind, a faulty
facial recognition match led to a Michigan man's arrest for a crime he
did not
commit.}\label{in-what-may-be-the-first-known-case-of-its-kind-a-faulty-facial-recognition-match-led-to-a-michigan-mans-arrest-for-a-crime-he-did-not-commit-1}}

Hosted by Annie Brown, produced by Lynsea Garrison, Austin Mitchell and
Daniel Guillemette, and edited by Lisa Tobin and Larissa Anderson

Transcript

transcript

Back to The Daily

bars

0:00/28:13

-0:00

transcript

\hypertarget{wrongfully-accused-by-an-algorithm-2}{%
\subsection{Wrongfully Accused by an
Algorithm}\label{wrongfully-accused-by-an-algorithm-2}}

\hypertarget{hosted-by-annie-brown-produced-by-lynsea-garrison-austin-mitchell-and-daniel-guillemette-and-edited-by-lisa-tobin-and-larissa-anderson-1}{%
\subsubsection{Hosted by Annie Brown, produced by Lynsea Garrison,
Austin Mitchell and Daniel Guillemette, and edited by Lisa Tobin and
Larissa
Anderson}\label{hosted-by-annie-brown-produced-by-lynsea-garrison-austin-mitchell-and-daniel-guillemette-and-edited-by-lisa-tobin-and-larissa-anderson-1}}

\hypertarget{in-what-may-be-the-first-known-case-of-its-kind-a-faulty-facial-recognition-match-led-to-a-michigan-mans-arrest-for-a-crime-he-did-not-commit-2}{%
\paragraph{In what may be the first known case of its kind, a faulty
facial recognition match led to a Michigan man's arrest for a crime he
did not
commit.}\label{in-what-may-be-the-first-known-case-of-its-kind-a-faulty-facial-recognition-match-led-to-a-michigan-mans-arrest-for-a-crime-he-did-not-commit-2}}

Monday, August 3rd, 2020

\begin{itemize}
\item
  michael barbaro\\
  From The New York Times, I'm Michael Barbaro. This is ``The Daily.''
\item
  {[}music{]}\\
  Today: Facial recognition is becoming an increasingly popular tool for
  solving crimes. The Daily's Annie Brown speaks to Kashmir Hill about
  how that software is not treating everybody equally.

  It's Monday, August 3.
\item
  kashmir hill\\
  I'm just going the tape record with an app that I use. Do you guys
  have any questions or concerns before we start talking about what
  happened?
\item
  robert williams\\
  No.
\item
  melissa williams\\
  No.
\end{itemize}

annie brown

OK. So where do you think we should start the story of this case,
Kashmir?

kashmir hill

The story started, for the Williams family, in January of 2020.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Melissa got the call first. I got the call from her.
\end{itemize}

kashmir hill

It's a Thursday afternoon in Farmington Hills, Michigan, which is just
outside of Detroit.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  So I picked up Julia from school. Regular Thursday.
\end{itemize}

kashmir hill

And Melissa Williams, a realtor, is driving home from work. She's
picking up her daughter.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  And so it was right around, like, 4 o'clock. And I got a call.
\end{itemize}

kashmir hill

And she gets a call from somebody who says they're a police officer.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  They immediately said, we're calling about Robert from an incident in
  2018. He needs to turn himself in. So I was confused off the bat.
\end{itemize}

kashmir hill

She is white. And her husband, Robert Williams, is Black.

\begin{itemize}
\item
  melissa williams\\
  And they said, we assume you're his baby mama or that you're not
  together anymore. And ---
\item
  kashmir hill\\
  What?
\item
  melissa williams\\
  Yeah. I said, that's my husband. And what is this regarding? And they
  said, we can't tell you. But he needs to come turn himself in. And I
  said, well, why didn't you call him? And they said, can't you just
  give him a message?
\end{itemize}

annie brown

Wait. So why is this officer calling her?

kashmir hill

She doesn't know why the officer is calling her. All she knows is that
the police want to be in touch with her husband. So she gives the
officer her husband's number. And then she calls Robert.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  And I said, I just got a really weird call. I was like, what did you
  do? Like, what is this about?
\end{itemize}

kashmir hill

And while they're talking, Robert Williams gets a call from the police
department.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Of course, I answered the other line. And he said he was a detective
  from Detroit and that I need to come turn myself in. So of course I'm
  like, for what? And he's like, well, I can't tell you over the phone.
  So I'm like, well, I can't turn myself in then.
\end{itemize}

kashmir hill

It was a couple of days before his birthday. So he thought maybe it was
a prank call. But it became pretty clear that the person was serious.

\begin{itemize}
\tightlist
\item
  robert williams\\
  About, uh, probably ten minutes later, I pull in the driveway.
\end{itemize}

kashmir hill

And when he pulls into his driveway, a police car pulls in behind him,
blocking him in. And two officers get out.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Yeah. So I get out of the car. And the driver, like, runs up. And he's
  like, are you Robert Williams? I'm like, yeah. He's like, you're under
  arrest. I'm like, no I'm not. And the guy comes up with, like, a white
  sheet of paper. And it's said ``felony warrant'' on the top,
  ``larceny.'' And I'm confused, like, isn't larceny stealing?
\end{itemize}

kashmir hill

His wife comes out with his two young daughters. And his oldest
daughter, who's 5, is watching this happen.

\begin{itemize}
\tightlist
\item
  robert williams\\
  I said, Juju (ph), go back in the house. I'll be back in a minute.
  They're just making a mistake. The guy, the other cop, is behind me
  with his handcuffs out already. So he's like, come on, man. You
  already --- you know the drill. And I'm like, what?
\end{itemize}

kashmir hill

The officers arrest him. They have to use two pairs of handcuffs to get
his hands behind his back, because he's a really big guy.

\begin{itemize}
\tightlist
\item
  robert williams\\
  We started moving seats around, trying to get me in the back of this
  little bitty Impala. And off we go.
\end{itemize}

kashmir hill

And then they drive to the detention center.

{[}music{]}

\begin{itemize}
\item
  robert williams\\
  I took fingerprints. I took ---
\item
  kashmir hill\\
  Your mug shot.
\item
  robert williams\\
  Mug shot pictures.
\end{itemize}

kashmir hill

Then he's put in a cell to sleep overnight.

\begin{itemize}
\item
  robert williams\\
  At this point, I'm in a holding cell with two other guys. And they're
  like, what you in here for? And I'm like, I don't know.
\item
  kashmir hill\\
  So when do you actually find out why you've been arrested, beyond this
  kind of vague larceny?
\item
  robert williams\\
  Um, so --- well, maybe like noon the next day.
\end{itemize}

kashmir hill

Around noon the next day, he is taken to an interrogation room. And
there's two detectives there. And they have three pieces of paper face
down in front of them. And they turn over the first sheet of paper. And
it's a picture from a surveillance video of a large Black man standing
in a store, wearing a red Cardinals cap and a black jacket. And the
detectives ask, is this you?

\begin{itemize}
\tightlist
\item
  robert williams\\
  I laugh a little bit, and I say, no, that's not me. So then he turns
  over another paper.
\end{itemize}

kashmir hill

And they turn over a second piece of paper, which is just a close up of
that same guy's face.

\begin{itemize}
\tightlist
\item
  robert williams\\
  And he says I guess that's not you either. And I said, no. This is not
  me.
\end{itemize}

kashmir hill

So Robert picks the piece of paper up, holds it next to his own face ---

\begin{itemize}
\tightlist
\item
  robert williams\\
  I was like, what you think, all Black men look alike?
\end{itemize}

kashmir hill

--- and says, do all Black men look the same to you?

annie brown

So what's your understanding, Kashmir, of what happened to bring Robert
Williams into that police department?

kashmir hill

So Robert Williams had no idea what was happening. But two years
earlier, in October 2018, a man who was not him walked into a Shinola
store in downtown Detroit. And Shinola is kind of like a high-end store
that sells expensive watches and bikes. So this man came in. He was
there for a few minutes. He stole five watches worth \$3,800 and walked
out. None of the employees there actually saw the theft occur. And so
they had to review the surveillance footage. And they found the moment
it happened. So they sent that surveillance footage picture that Robert
Williams had been shown to the Detroit police. And the police turned to
what a lot of police turn to these days when they have a suspect that
they don't recognize --- a facial recognition system. So they ran a
search on this, what they call a probe image, this picture for the
surveillance video, which is really grainy. It's not a very good photo.
And the way these systems work is that they have access not just to mug
shots but also to driver's license photos. You get a bunch of different
results. And there's a human involved who decides which of the results
looks the most like the person who committed the crime.

annie brown

Mm. So you're saying the facial recognition algorithm basically created
a lineup of potential suspects. And then from that lineup, someone picks
the person that they think looks the most like the man in the
surveillance video.

kashmir hill

Right. And so that is how they wound up arresting Robert Williams.

{[}music{]}

So back in this room, the two detectives now have the real Robert
Williams in front of them. And he doesn't look like this guy.

\begin{itemize}
\tightlist
\item
  robert williams\\
  You know, they sat back and looked at each other and was like, with
  the oops face, right? Says, so I guess the computer got it wrong too.
\end{itemize}

kashmir hill

And so they kind of leaned back and said, I guess the computer got it
wrong.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Well, the computer got it wrong is what threw me off. And I'm like,
  computer got it wrong?
\end{itemize}

annie brown

And what is the significance of that statement, ``that the computer got
it wrong``?

kashmir hill

So this was an admission by the detectives that it was a computer that
had pointed the finger at Robert Williams. And that's significant,
because this is the first documented case of an innocent person being
arrested because of a flawed facial recognition match.

{[}music{]}

annie brown

And just to put all of this into context for a second, the last time
that you and I talked, Kashmir, we were talking about a different
development in facial recognition --- this new algorithm being used by
some police departments that drew from pictures all over social media
and all over the internet to make a kind of super algorithm. But the
fear wasn't that it wasn't accurate. It was almost that it was too
accurate, that it knew too much. But what you're describing is something
altogether different. Right?

kashmir hill

So when we talk about facial recognition, we often think of it as a
monolith, that there's kind of one facial recognition. But in fact,
there's a bunch of different companies that all have their own
algorithms. And some work well. And some don't work well. And some work
well sometimes. Like, identifying a really clear photo is a lot easier
than identifying surveillance footage.

annie brown

And why wouldn't police departments be using the most sophisticated, the
most kind of up-to-date version of this software?

kashmir hill

I mean, this is where you run into just bureaucracy. Right? You have
contracts with companies that go back years and just a lot of different
vendors. And so in this case, I tried to figure out exactly whose
algorithms were responsible for Robert Williams getting arrested. And I
had to really dig down. And I discovered the police had no idea. You
know, they contract out to a company called DataWorks Plus. And
DataWorks Plus contracts out to two other companies called N.E.C. and
Rank One that actually supply the algorithm. It's this whole chain of
companies that are involved. And there is no standardized testing.
There's no one really regulating this. There's just nobody saying which
algorithms, you know, pass the test to be used by law enforcement. It's
just up to police officers, who, for the most part, seem to be just
testing it in the field to see if it works, if it's identifying the
right people.

But the really big problem is that these systems have been proven to be
biased.

{[}music{]}

michael barbaro

We'll be right back.

annie brown

So, Kashmir, help me understand how an algorithm can become biased.

kashmir hill

Well, the bias tends to come from how the algorithm is trained. And
these algorithms tend to be trained by basically feeding them with a
bunch of images of people. But the problem with the algorithms is that
they tended to be trained with non-diverse data sets.

annie brown

Mm.

kashmir hill

So one good example is that many of the algorithms used by law
enforcement in the U.S., by government in the U.S., are very good at
recognizing white men and not as good at recognizing Black people or
Asian-Americans. But if you go to an algorithm from a company in China,
where they fed it with a lot of images of Asian people, they're really
good at recognizing Asian people and not as good at recognizing white
men. So you can just, you can see the biases that come in from the kind
of data that we feed into these systems.

annie brown

And is this a widely agreed upon reality --- that because of these
methods, the algorithms used in the U.S. are just worse at identifying
faces that aren't white men?

kashmir hill

Yeah. A few years ago, an M.I.T. researcher did this study and found
that facial recognition algorithms were biased to be able to recognize
white men better. And shortly after that, NIST, the National Institute
of Standards and Technology, decide to run its own study on this. And it
found the same thing. It looked at over 100 different algorithms. And it
found that they were biased. And actually, the two algorithms that were
at the heart of this case --- the Robert Williams's case --- were in
that study.

annie brown

So the algorithm that was used by this police department was actually
studied by the federal government and was proven to be biased against
faces like Robert Williams.

kashmir hill

Exactly.

annie brown

So given these widely-understood problems with these algorithms, how can
police departments justify continuing to use them?

kashmir hill

So police departments are aware of the bias problem. But they feel that
face recognition is just too valuable a tool in their tool set to solve
crimes. And their defense is that they never arrest somebody based on
facial recognition alone, that facial recognition is only what they call
an investigative lead. It doesn't supply probable cause for arrest.

So what police are supposed to do is they get a facial recognition
match, and you're supposed to do more investigating. So you could go to
the person's social media account and see if there are other photos of
them wearing the same clothes that they were wearing on the day they
committed this crime. Or, you know, you can try to get proof that they
were in that part of town on the day that the theft occurred. You know,
try to get location data. Basically, find other evidence that this
person is the person that committed the crime.

The detectives just went to the woman who had spotted the theft on the
video and showed her a photo of six people --- they call it a six pack.
And she said Robert Williams looked the most like the person that was in
the video.

annie brown

Mm. So they're supposed to use the facial recognition match as a kind of
clue. And then the protocol calls for them to do more police work to
verify it. But in this case, they basically just had someone watch the
video and then identify Robert Williams as the one who looks most like
the guy in the video.

kashmir hill

Yeah, they just did facial recognition a second time, but with a human
who's not actually trained. And they didn't do any other investigating.
Based on that, they went out and they arrested Mr. Williams.

annie brown

But if the police had done their job correctly --- if they had looked
into his social media accounts, if they had tried to get his location
data from his phone records, essentially surveilling him more closely
--- wouldn't that be its own sort of violation? Just because their
technology wrongfully identified this man, he gets more closely watched
by the police without his knowledge.

kashmir hill

Right. And this is actually what police asked the facial recognition
vendors to do. They want to have more, what you call false positives,
because they want to have the greatest pool of possible suspects that
they can, because they want to find the bad guy.

annie brown

Huh.

kashmir hill

But there's a real toll from that.

annie brown

Hmm.

kashmir hill

I just, you know, as a person who's been reporting on technology for a
decade, I just think people trust computers. And even when we know
something is flawed, if it's a computer telling us to do it, we just
think it's right. And this is why we always used to see, for a long
time, when mapping technology was first being developed and it wasn't
that great, you know, people would drive into lakes. They would drive
over cliffs, because a mapping app said, you're supposed to go straight
here.

annie brown

Right.

kashmir hill

And even though they could look and see that their life is going to be
in danger, they would think, well, this app must know what it's talking
about. That's facial recognition now. And when I was reporting this
story, all the experts I talked to said this is surely not the first
case where somebody has been mistakenly --- an innocent person has been
mistakenly arrested because of a bad face recognition match. But usually
people don't find out about it. Police don't tell people that they're
there because of face recognition.

annie brown

Hmm.

kashmir hill

Usually, when they charge them, they'll just say they were identified
through investigative means. It's kind of a vague, ``There were clues
that pointed at you.'' In that way, Robert's case was unusual, because
there was so little evidence against him. They basically had to tell him
that they used facial recognition, you know, to put him there.

annie brown

Right. They showed him what most people don't get to see, which is this
false match between his photo and the photo of the crime.

kashmir hill

Right.

annie brown

And what's happened since Robert was arrested?

kashmir hill

So Robert had to hire a lawyer to defend himself. But when he went to
the hearing, the prosecutor decided to drop the case. But they dropped
it without prejudice, which meant that they could charge him again.

annie brown

For the same crime?

kashmir hill

With the same crime. So as I was reporting out the story, you know, I
went to the prosecutor's office. I went to the Detroit Police
Department. And I said, you know, what happened here? Did you have any
other evidence? This just seems like a clear misfire and misuse of
facial recognition. And everyone involved was pretty defensive and said,
well, you know, there might be more evidence that proves that Robert
Williams did it.

But after the story came out, everybody's tune changed dramatically.
Prosecutors office apologized, said that Robert Williams shouldn't have
spent any time in jail. The Detroit Police Department said this was a
horrible investigation. The police officers involved just did this all
wrong. This isn't how it's supposed to work. And they said that Robert
Williams would have his information expunged from the system --- his mug
shot, his DNA. And they personally apologized to the Williams family,
though the Williams family told me that no one ever actually called them
to personally apologize.

annie brown

But he can no longer be charged in the future for this crime?

kashmir hill

That's exactly right.

annie brown

And what about their use of facial recognition software? Has there been
any change there?

kashmir hill

So one thing the Detroit Police Department said was, well, this was a
case that predates this new policy we have that says, you know, we're
only supposed to be using facial recognition for violent crimes.

annie brown

Hmm. And what do you make of that? Why only use this tool for that?

kashmir hill

Well, their justification is that when it comes to violent crimes, when
it comes to murder, you know, rape, they need to solve these cases. And
they'll use any clue they can to do it, including facial recognition.
But I think about something that Robert's wife said.

\begin{itemize}
\tightlist
\item
  melissa williams\\
  When they pulled up to our house, they were already combative on the
  phone. They were aggressive in the doorway to me. What if he had been
  argumentative? If he'd been defensive, if he hadn't complied, you
  know, what could that have turned into in our yard? Like, it could
  have went a different way. And the recent news has shown us that it
  definitely could have went a different way.
\end{itemize}

{[}music{]}

\begin{itemize}
\item
  kashmir hill\\
  Do you feel like there's a shame to this, that the police arrested you
  even though you did nothing?
\item
  robert williams\\
  It's a little humiliating. You know, it's not something that easily
  rolls off the tongue, like, oh yeah, and guess what? I got arrested.
\end{itemize}

{[}music{]}

annie brown

And what about for Robert himself? What has life been like for him after
the arrest?

kashmir hill

So this was very embarrassing for him and kind of painful in some ways.
So he had a perfect attendance at work until that day that he was
arrested. And his wife had to email his boss and say that they had a
family emergency and that he couldn't show up that day. Once he did tell
his boss what happened, his boss said, you know, you don't want to tell
other people at work. You know, it could be bad for you. The night he
got home, his daughter --- his 5-year-old was still awake.

\begin{itemize}
\tightlist
\item
  robert williams\\
  Julia was still up. And I was like, what are you doing up? And she was
  like, I'm waiting for you. And I was like, I told you I'll be right
  back. And she was like, you didn't come right back though. So I just
  kept telling her that they made a mistake. And it just took longer
  than we expected. But ---
\end{itemize}

kashmir hill

She started wanting to play cops and robbers. And she would always
pretend like he was the robber who stole something, and she would need
to lock him up in the living room.

annie brown

Hmm.

\begin{itemize}
\item
  melissa williams\\
  Oh yeah. She told us that she told one of her --- Jackson, her friend
  at school. And we weren't sure, did she tell her teacher? Did she tell
  her friends? We were not sure. And we didn't know what to say to
  people. Like, just bring it up out of nowhere, like, oh yeah, in case
  anyone mentioned it, he was arrested, but he didn't do anything.
\item
  kashmir hill\\
  Has this made you look back to see where you --- like, where you were
  October 2018?
\item
  robert williams\\
  Yeah. I pulled it up. At the time, I was on my Facebook or on my
  Instagram Live.
\end{itemize}

kashmir hill

He has since looked back and realized that he had posted to Instagram at
basically the same time as the shoplifting was occurring. He was driving
home from work, and a song came on the radio that his mother loved: the
song ``We Are One'' by Maze and Frankie Beverly.

\begin{itemize}
\tightlist
\item
  robert williams\\
  I was singing songs on my way home in the car.
\end{itemize}

annie brown

So if the cops had looked in to his social media, if they had tried to
verify that it was possible that he could have committed this crime,
they could have found this video.

kashmir hill

Right. If the police had done a real investigation, they would have
found out he had an alibi that day.

\begin{itemize}
\tightlist
\item
  archived recording\\
  {[}``WE ARE ONE'' PLAYING{]}
\end{itemize}

annie brown

Kashmir, thank you so much.

kashmir hill

Thank you.

{[}music{]}

michael barbaro

We'll be right back.

Here's what else you need to know today. Federal unemployment benefits
have expired for tens of millions of Americans after Congress failed to
reach a deal to renew them last week.

\begin{itemize}
\item
  archived recording\\
  So what do you say to those 30 million Americans who are now without
  federal unemployment help?
\item
  archived recording (nancy pelosi)\\
  I say to them, talk to President Trump. He's the one who is standing
  in the way of that. We have been for the \$600. They have a \$200
  proposal, which does not meet the needs of America's working families.
  And ---
\end{itemize}

michael barbaro

In interviews on Sunday with ABC's ``This Week,'' House Speaker Nancy
Pelosi blamed Republicans for demanding a drastic cut in the weekly
benefit, while Treasury Secretary Steve Mnuchin claimed that the \$600
payments risked overpaying unemployed workers.

\begin{itemize}
\item
  archived recording\\
  So you do think it is a disincentive to find a job if you have that
  extra \$600?
\item
  archived recording (steven mnuchin)\\
  There's no question. In certain cases where we're paying people more
  stay home than to work, that's created issues in the entire economy.
\end{itemize}

michael barbaro

And The Times reports that July was a devastating month for the pandemic
in the U.S. The country recorded nearly 2 million new infections, twice
as many as any previous month.

\begin{itemize}
\tightlist
\item
  archived recording (deborah birx)\\
  I want to be very clear. What we're seeing today is different from
  March and April. It is extraordinarily widespread. It's into the rural
  as equal urban areas.
\end{itemize}

michael barbaro

In an interview on Sunday with CNN, Dr. Deborah Birx, a top White House
adviser on the pandemic, acknowledged that the United States has failed
to contain the virus.

\begin{itemize}
\tightlist
\item
  archived recording (deborah birx)\\
  And to everybody who lives in a rural area, you are not immune or
  protected from this virus. And that's why we keep saying, no matter
  where you live in America, you need to wear a mask and socially
  distance. Do the personal hygiene ---
\end{itemize}

michael barbaro

That's it for ``The Daily.'' I'm Michael Barbaro. See you tomorrow.

{[}music{]}

Previous

More episodes ofThe Daily

\href{https://www.nytimes3xbfgragh.onion/2020/08/04/podcasts/the-daily/mail-in-voting-president-trump.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/30/us/politics/04daily/30trump-election1-thumbLarge.jpg}}

August 4, 2020Is the U.S. Ready to Vote by Mail?

\href{https://www.nytimes3xbfgragh.onion/2020/08/03/podcasts/the-daily/algorithmic-justice-racism.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/06/24/business/03daily/24michigan-arrest1-thumbLarge.jpg}}

August 3, 2020~~~ 28:13Wrongfully Accused by an Algorithm

\href{https://www.nytimes3xbfgragh.onion/2020/08/02/podcasts/the-daily/on-female-rage.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2018/01/21/magazine/21mag-femaleanger1-copy/21mag-femaleanger1-thumbLarge.jpg}}

August 2, 2020The Sunday Read: `On Female Rage'

\href{https://www.nytimes3xbfgragh.onion/2020/07/31/podcasts/the-daily/vanessa-guillen-military-metoo.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/12/us/politics/31daily/00dc-army-metoo-thumbLarge.jpg}}

July 31, 2020A \#MeToo Moment in the Military

\href{https://www.nytimes3xbfgragh.onion/2020/07/30/podcasts/the-daily/congress-facebook-amazon-google-apple.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/30/reader-center/30daily/merlin_175077825_5ebc931b-baa1-489a-960c-34e4d845e997-thumbLarge.jpg}}

July 30, 2020~~~ 35:19The Big Tech Hearing

\href{https://www.nytimes3xbfgragh.onion/2020/07/29/podcasts/the-daily/china-trump-foreign-policy.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/26/world/29daily/00china-us-clash1-thumbLarge.jpg}}

July 29, 2020~~~ 28:40Confronting China

\href{https://www.nytimes3xbfgragh.onion/2020/07/28/podcasts/the-daily/unemployment-benefits-coronavirus.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/23/business/28daily/23virus-uiexplain1-thumbLarge.jpg}}

July 28, 2020~~~ 26:13Why \$600 Checks Are Tearing Republicans Apart

\href{https://www.nytimes3xbfgragh.onion/2020/07/27/podcasts/the-daily/new-york-hospitals-covid.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/27/world/27daily-hospitals/27daily-hospitals-thumbLarge.jpg}}

July 27, 2020~~~ 33:28The Mistakes New York Made

\href{https://www.nytimes3xbfgragh.onion/2020/07/26/podcasts/the-daily/the-accusation-the-sunday-read.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/03/22/magazine/26audm-2/22mag-titleix-thumbLarge.jpg}}

July 26, 2020The Sunday Read: `The Accusation'

\href{https://www.nytimes3xbfgragh.onion/2020/07/24/podcasts/the-daily/mlb-baseball-season-coronavirus.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/22/sports/24daily/22mlb-previewlede1-thumbLarge.jpg}}

July 24, 2020~~~ 45:34The Battle for a Baseball Season

\href{https://www.nytimes3xbfgragh.onion/2020/07/23/podcasts/the-daily/portland-protests.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/22/us/23daily-image/22portland-tactics02-thumbLarge.jpg}}

July 23, 2020~~~ 30:04The Showdown in Portland

\href{https://www.nytimes3xbfgragh.onion/2020/07/22/podcasts/the-daily/school-reopenings-coronavirus.html?action=click\&module=audio-series-bar\&region=header\&pgtype=Article}{\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/07/12/science/22daily/00virus-schools-reopen01-thumbLarge.jpg}}

July 22, 2020~~~ 27:24The Science of School Reopenings

\href{https://www.nytimes3xbfgragh.onion/column/the-daily}{See All
Episodes ofThe Daily}

Next

Aug. 3, 2020

\begin{itemize}
\item
\item
\item
\item
\item
\item
\end{itemize}

\emph{\textbf{Listen and subscribe to our podcast from your mobile
device:}}\\
\textbf{\href{https://itunes.apple.com/us/podcast/the-daily/id1200361736?mt=2}{\emph{Via
Apple Podcasts}}} \emph{\textbf{\textbar{}}}
\textbf{\href{https://open.spotify.com/show/3IM0lmZxpFAY7CwMuv9H4g?si=SfuMSC55R1qprFsRZU3_zw}{\emph{Via
Spotify}}} \emph{\textbf{\textbar{}}}
\textbf{\href{http://www.stitcher.com/podcast/the-new-york-times/the-daily-10}{\emph{Via
Stitcher}}}

Facial recognition is becoming an increasingly central component of
police departments' efforts to solve crimes. But can algorithms harbor
racial bias?

\textbf{On today's episode:}

\begin{itemize}
\tightlist
\item
  Annie Brown, a producer for The New York Times, speaks with
  \href{https://www.nytimes3xbfgragh.onion/by/kashmir-hill}{Kashmir
  Hill}, a technology reporter, about her interview with Robert
  Julian-Borchak Williams, who was arrested after being misidentified as
  a criminal by an algorithm.
\end{itemize}

\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2020/06/24/business/03daily/24michigan-arrest1-articleLarge.jpg?quality=75\&auto=webp\&disable=upscale}

\textbf{Background reading:}

\begin{itemize}
\tightlist
\item
  In response to
  \href{https://www.nytimes3xbfgragh.onion/2020/06/24/technology/facial-recognition-arrest.html}{Mr.
  Williams's story being published by The New York Times}, the Wayne
  County prosecutor's office said that he could have the case and his
  fingerprint data expunged.
\end{itemize}

\emph{Tune in, and tell us what you think. Email us at}
\href{mailto:thedaily@NYTimes.com}{\emph{thedaily@NYTimes.com}}\emph{.
Follow Michael Barbaro on Twitter:}
\href{https://twitter.com/mikiebarb}{\emph{@mikiebarb}}\emph{. And if
you're interested in advertising with ``The Daily,'' write to us at}
\href{mailto:thedaily-ads@NYTimes.com}{\emph{thedaily-ads@NYTimes.com}}\emph{.}

Kashmir Hill contributed reporting.

``The Daily'' is made by Theo Balcomb, Andy Mills, Lisa Tobin, Rachel
Quester, Lynsea Garrison, Annie Brown, Clare Toeniskoetter, Paige
Cowett, Michael Simon Johnson, Brad Fisher, Larissa Anderson, Wendy
Dorr, Chris Wood, Jessica Cheung, Stella Tan, Alexandra Leigh Young,
Lisa Chow, Eric Krupke, Marc Georges, Luke Vander Ploeg, Kelly Prime,
Julia Longoria, Sindhu Gnanasambandan, M.J. Davis Lin, Austin Mitchell,
Neena Pathak, Dan Powell, Dave Shaw, Sydney Harper, Daniel Guillemette,
Hans Buetow, Robert Jimison, Mike Benoist, Bianca Giaever and Asthaa
Chaturvedi. Our theme music is by Jim Brunberg and Ben Landsverk of
Wonderly. Special thanks to Sam Dolnick, Mikayla Bouchard, Lauren
Jackson, Julia Simon, Mahima Chablani and Nora Keller.

Advertisement

\protect\hyperlink{after-bottom}{Continue reading the main story}

\hypertarget{site-index}{%
\subsection{Site Index}\label{site-index}}

\hypertarget{site-information-navigation}{%
\subsection{Site Information
Navigation}\label{site-information-navigation}}

\begin{itemize}
\tightlist
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014792127-Copyright-notice}{~2020~The
  New York Times Company}
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://www.nytco.com/}{NYTCo}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115015385887-Contact-Us}{Contact
  Us}
\item
  \href{https://www.nytco.com/careers/}{Work with us}
\item
  \href{https://nytmediakit.com/}{Advertise}
\item
  \href{http://www.tbrandstudio.com/}{T Brand Studio}
\item
  \href{https://www.nytimes3xbfgragh.onion/privacy/cookie-policy\#how-do-i-manage-trackers}{Your
  Ad Choices}
\item
  \href{https://www.nytimes3xbfgragh.onion/privacy}{Privacy}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893428-Terms-of-service}{Terms
  of Service}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893968-Terms-of-sale}{Terms
  of Sale}
\item
  \href{https://spiderbites.nytimes3xbfgragh.onion}{Site Map}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us}{Help}
\item
  \href{https://www.nytimes3xbfgragh.onion/subscription?campaignId=37WXW}{Subscriptions}
\end{itemize}
